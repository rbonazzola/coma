{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os; os.chdir(\"/home/ubuntu/coma\")\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "import yaml\n",
    "from copy import deepcopy\n",
    "import json\n",
    "import re\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import time, datetime\n",
    "from cardiac_mesh import CardiacMesh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some constant definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "coma_dir = \"/home/ubuntu/coma/\"\n",
    "yaml_subdir = os.path.join(coma_dir, \"yamls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing\n",
    "Generate two numpy files with meshes (for training and testing, respectively), from the corresponding VTK files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select a YAML file containing all the parameters necessary to define the model and the training process, as well as the paths where the input will be fetched and the output will be stored.\n",
    "config.yaml is a reference yaml file, and other yaml files can be created from this one in the cell below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the reference yaml to generate yamls with other parameters\n",
    "This cell builds a set of yaml files for a grid of parameters, based on a reference yaml file. (If there are no new files to generate it can be commented out or skipped.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = yaml.safe_load(open(os.path.join(yaml_subdir, \"config.yaml\")))\n",
    "\n",
    "data_files = {\n",
    "    \"LV\": \"/MULTIX/DATA/INPUT/disk_2/coma/Cardio/meshes/numpy_files/LV_all_subjects/train.npy\",\n",
    "    \"LV_endo\": \"/MULTIX/DATA/INPUT/disk_2/coma/Cardio/meshes/numpy_files/LV_endo_all_subjects/LVED_all_subjects.npy\",\n",
    "    \"RV\": \"/MULTIX/DATA/INPUT/disk_2/coma/Cardio/meshes/numpy_files/RV/RV_all.npy\"\n",
    "}\n",
    "\n",
    "ids_files = {\n",
    "    \"LV\": \"/MULTIX/DATA/INPUT/disk_2/coma/Cardio/meshes/numpy_files/LV_all_subjects/LVED_all_subjects_subj_ids.txt\",\n",
    "    \"LV_endo\": \"/MULTIX/DATA/INPUT/disk_2/coma/Cardio/meshes/numpy_files/LV_endo_all_subjects/LVED_all_subjects_subj_ids.txt\",\n",
    "    \"RV\": \"/MULTIX/DATA/INPUT/disk_2/coma/Cardio/meshes/numpy_files/RV/RV_all_subject_ids.txt\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kk = np.load(data_files[\"LV\"], allow_pickle=True)\n",
    "# ids=[x.strip() for x in open(ids_files[\"RV\"])]\n",
    "# \n",
    "# pp = np.stack([k for k in kk if k.shape == (3478,3)]) #np.vstack([kk ])\n",
    "# ids = [ids[i] for i,k in enumerate(kk) if k.shape == (3478,3)] #np.vstack([kk ])\n",
    "# np.save(data_files[\"RV\"]+\"_\", arr=pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nTraining = [100, 200, 400, 800, 1600, 3200, 6400]\n",
    "nTraining = [1600, 3200] #, 200, 400, 800, 1600, 3200, 6400]\n",
    "# nTraining = [1600, 3200, 6400]\n",
    "nTest = 2000\n",
    "\n",
    "Fs = [\n",
    "  # [16, 32, 32, 64],\n",
    "  # [16, 32, 64, 128],\n",
    "  # [8, 16, 32, 64],\n",
    "  [8, 16, 32, 64, 128] #,\n",
    "]\n",
    "\n",
    "dir_pattern = \"{}__ds_{}__nz_{}__reg_{}__lr_{}\"\n",
    "dss = [ \n",
    "  # [4, 4, 4, 2], \n",
    "  # [4, 4, 3, 2], \n",
    "  # [4, 4, 2, 2], \n",
    "  [2, 2, 2, 2, 2] #,\n",
    "  # [4, 3, 3, 3]\n",
    "] # downsampling factors\n",
    "\n",
    "regs = [1e-4, 5e-4] # [1e-5, 1e-4, 1e-3]\n",
    "nzs = [8] # [4, 8, 16, 32]\n",
    "chambers = [\"LV\", \"RV\"]#, \"LV_endo\"]\n",
    "nepochss = [500]\n",
    "learning_rates = [1e-3, 1e-2] #, 5e-2]\n",
    "decay_rates = [0.99] #, 0.995, 0.999]\n",
    "losses = ['l1', 'l2']\n",
    "batch_sizes = [16, 64]\n",
    "\n",
    "parameters_grid = [\n",
    "  dss, regs, nzs, chambers,\n",
    "  nepochss, learning_rates, decay_rates,\n",
    "  nTraining, losses, Fs\n",
    "]\n",
    "\n",
    "for comb in itertools.product(*parameters_grid):\n",
    "    \n",
    "    ds = comb[0]\n",
    "    reg = comb[1]\n",
    "    nz = comb[2]\n",
    "    chamber = comb[3]\n",
    "    nepochs = comb[4]\n",
    "    learning_rate = comb[5]\n",
    "    decay_rate = comb[6]\n",
    "    nTr = comb[7]\n",
    "    which_loss = comb[8]\n",
    "    F = comb[9]\n",
    "    \n",
    "    config['num_epochs'] = nepochs\n",
    "    config['ds_factors'] = ds\n",
    "    config['partition'] = chamber\n",
    "    config['regularization'] = reg\n",
    "    config['learning_rate'] = learning_rate\n",
    "    config['decay_rate'] = decay_rate\n",
    "    config['nz'] = nz\n",
    "    config['nTraining'] = nTr\n",
    "    config['nVal_max'] = 200\n",
    "    config['nVal_fraction'] = 0.20\n",
    "    config['ids_file'] = ids_files[chamber]\n",
    "    config['data_file'] = data_files[chamber]\n",
    "    config['which_loss'] = which_loss\n",
    "    config['F'] = F\n",
    "    config['K'] = [6] * len(F)\n",
    "    \n",
    "    # config['data_dir'] = config['data_dir']/{}'.format(config['partition'])\n",
    "  \n",
    "    config['dir_name'] = dir_pattern.format(\n",
    "                              chamber,\n",
    "                              \"\".join([str(x) for x in ds]), \n",
    "                              nz, reg, learning_rate\n",
    "                          )\n",
    "    \n",
    "    hsh = hash(tuple([config[k] for k in config.keys() if not isinstance(config[k], list)])) % 1000000\n",
    "    config['hash'] = hsh\n",
    "    yaml.dump(config, open(os.path.join(yaml_subdir, \"config__{}__{}.yaml\".format(config['dir_name'], hsh)), \"w\"))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def to_tuple(x):\n",
    "    return tuple(x) if isinstance(x, list) else x\n",
    "\n",
    "def to_list(x):\n",
    "    return list(x) if isinstance(x, tuple) else x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_files = {x:yaml.safe_load(open(os.path.join(yaml_subdir, x))) for x in os.listdir(yaml_subdir) if x != \"config.yaml\"}\n",
    "ref_conf = config_files[config_files.keys()[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dif_cols = [ col for col in ref_conf.keys() if any([config_files[fn][col] != ref_conf[col] for fn in config_files]) ]\n",
    "\n",
    "params_df = pd.DataFrame(\n",
    "    [[config_files[x][col] for col in dif_cols] for x in config_files], \n",
    "    columns=dif_cols\n",
    ")\n",
    "\n",
    "dif_cols.remove(\"hash\")\n",
    "dif_cols.remove(\"dir_name\")\n",
    "dif_cols.remove(\"ids_file\")\n",
    "dif_cols.remove(\"data_file\")\n",
    "\n",
    "selection_w = {}\n",
    "# for col in [ x for x in dif_cols if x != \"hash\" ]:\n",
    "\n",
    "# params_df.loc[:,dif_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nTraining\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5edd0318f1b44fcea2c117cd9e9ce5e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SelectMultiple(description=u'nTraining', options=(1600, 3200), value=())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regularization\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69d7ee636e4e468ea0dfe4d6e9382d90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SelectMultiple(description=u'regularization', options=(0.0005, 0.0001), value=())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5355ffd438314471b4018b01dc2bd7aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SelectMultiple(description=u'learning_rate', options=(0.001, 0.01), value=())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "which_loss\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e034338a8854bdbb461b314c97a667f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SelectMultiple(description=u'which_loss', options=('l2', 'l1'), value=())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partition\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a262300854814d0e8d40202f559fe855",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SelectMultiple(description=u'partition', options=('LV', 'RV'), value=())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for col in dif_cols:\n",
    "  print(col)\n",
    "  selection_w[col] = widgets.SelectMultiple(\n",
    "      options=set([\n",
    "          to_tuple(\n",
    "              yaml.safe_load(\n",
    "                 open(os.path.join(yaml_subdir, x))\n",
    "              )[col]\n",
    "          ) for x in config_files\n",
    "      ]), \n",
    "      description=col)\n",
    "  display(selection_w[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_values = {x: [to_list(y) for y in selection_w[x].value] for x in selection_w}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nTraining</th>\n",
       "      <th>regularization</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>which_loss</th>\n",
       "      <th>partition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1600</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>l1</td>\n",
       "      <td>LV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3200</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.001</td>\n",
       "      <td>l1</td>\n",
       "      <td>LV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>l1</td>\n",
       "      <td>RV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1600</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.010</td>\n",
       "      <td>l1</td>\n",
       "      <td>RV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1600</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.010</td>\n",
       "      <td>l1</td>\n",
       "      <td>RV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3200</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.010</td>\n",
       "      <td>l1</td>\n",
       "      <td>LV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3200</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.001</td>\n",
       "      <td>l1</td>\n",
       "      <td>RV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1600</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.010</td>\n",
       "      <td>l1</td>\n",
       "      <td>LV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1600</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.010</td>\n",
       "      <td>l1</td>\n",
       "      <td>LV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>l1</td>\n",
       "      <td>LV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1600</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.001</td>\n",
       "      <td>l1</td>\n",
       "      <td>LV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.010</td>\n",
       "      <td>l1</td>\n",
       "      <td>RV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3200</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.010</td>\n",
       "      <td>l1</td>\n",
       "      <td>RV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1600</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.001</td>\n",
       "      <td>l1</td>\n",
       "      <td>RV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1600</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>l1</td>\n",
       "      <td>RV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.010</td>\n",
       "      <td>l1</td>\n",
       "      <td>LV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    nTraining  regularization  learning_rate which_loss partition\n",
       "2        1600          0.0001          0.001         l1        LV\n",
       "3        3200          0.0005          0.001         l1        LV\n",
       "4        3200          0.0001          0.001         l1        RV\n",
       "5        1600          0.0005          0.010         l1        RV\n",
       "7        1600          0.0001          0.010         l1        RV\n",
       "8        3200          0.0005          0.010         l1        LV\n",
       "9        3200          0.0005          0.001         l1        RV\n",
       "11       1600          0.0005          0.010         l1        LV\n",
       "14       1600          0.0001          0.010         l1        LV\n",
       "15       3200          0.0001          0.001         l1        LV\n",
       "17       1600          0.0005          0.001         l1        LV\n",
       "18       3200          0.0001          0.010         l1        RV\n",
       "22       3200          0.0005          0.010         l1        RV\n",
       "25       1600          0.0005          0.001         l1        RV\n",
       "26       1600          0.0001          0.001         l1        RV\n",
       "29       3200          0.0001          0.010         l1        LV"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_df.apply(axis=1, func = lambda row: [row[x] in selected_values[x] for x in selected_values])\n",
    "yaml_files_df = params_df[params_df.apply(axis=1, func = lambda row: all([row[x] in selected_values[x] for x in selected_values]))]\n",
    "yaml_files_df[dif_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block creates a text file containing the names of a set of YAML files select using the above widgets. This allow to select a series of sets of parameters describing the network architecture and its training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"runs_yamls.txt\", \"w\") as ff:\n",
    "    for row_index in range(yaml_files_df.shape[0]):                \n",
    "        yaml_file = \"config__{}__{}.yaml\".format(\n",
    "            yaml_files_df.iloc[row_index]['dir_name'], \n",
    "            yaml_files_df.iloc[row_index]['hash']\n",
    "        )\n",
    "        # print(yaml_file)\n",
    "        ff.write(\"%s\\n\" % yaml_file)\n",
    "    \n",
    "#  for chamber in w_chamber.value:\n",
    "#    for ds in w_ds.value:\n",
    "#      for nz in w_nz.value:\n",
    "#        for reg in w_reg.value:\n",
    "#          for lr in w_lr.value:\n",
    "#              yaml_file = \"config__{}.yaml\".format(dir_pattern).format(\n",
    "#                           chamber, \n",
    "#                           ds.replace(\",\", \"\"), \n",
    "#                           nz, reg, lr)\n",
    "#              yaml_file = os.path.join(yaml_subdir, yaml_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = yaml.safe_load(open(\"yamls/config__RV__ds_4422__nz_16__reg_0.0001__lr_0.001__730462.yaml\"))\n",
    "min(params['nVal_fraction'] * params['nTraining'], params['nVal_max'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Tensorflow model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Path definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"/MULTIX/DATA/INPUT/disk_2/coma/Cardio/output/\"\n",
    "checkpoints_dir = \"{}/checkpoints\".format(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_yaml = widgets.Dropdown(\n",
    "    options= [\n",
    "        x for x in os.listdir(yaml_subdir) \n",
    "          if x.endswith(\".yaml\")\n",
    "            and x.startswith(\"config\")\n",
    "    ],\n",
    "    description='Yaml File',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "w_yaml = widgets.Dropdown(\n",
    "    options= [x.strip() for x in open(\"runs_yamls.txt\").readlines()],\n",
    "    description='Yaml File',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "\n",
    "display(w_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = yaml.safe_load(open(os.path.join(yaml_subdir, w_yaml.value)))\n",
    "#params['num_epochs'] = 500\n",
    "#params['nTraining'] = 1000\n",
    "#params['nTest'] = 2000\n",
    "#params['nVal'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import generate_model, run as train\n",
    "train(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select YAML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = re.compile(\"(.*)__ds_(.*)__nz_(.*)__reg_(.*)__lr_(.*)\")\n",
    "kk = [regex.match(x).groups() for x in os.listdir(checkpoints_dir)]\n",
    "available_runs = [\"config__{}__ds_{}__nz_{}__reg_{}__lr_{}\".format(*k) for k in kk]\n",
    "\n",
    "w_yaml = widgets.Dropdown(\n",
    "    options= [x + \".yaml\" for x in available_runs],\n",
    "    description='Yaml File',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "display(w_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = yaml.safe_load(open(os.path.join(yaml_subdir, w_yaml.value)))\n",
    "\n",
    "checkpoints_subdir = os.path.join(checkpoints_dir, config['dir_name'])\n",
    "\n",
    "from main2 import generate_model, run as train\n",
    "\n",
    "params = deepcopy(config)\n",
    "model = generate_model(params)\n",
    "model.p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model\n",
    "The cell below fits the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore session from last checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "last_chkpt = tf.train.latest_checkpoint(checkpoints_subdir)\n",
    "new_saver = tf.train.import_meta_graph(last_chkpt + \".meta\")\n",
    "new_saver.restore(sess, last_chkpt)\n",
    "\n",
    "data_prefix = \"data/coma_cardiac/{}\".format(config[\"partition\"])\n",
    "z_file_pattern = \"{}/latent_space/{{}}_latent_base_{{}}.csv\".format(output_dir)\n",
    "\n",
    "io_mapping = { subset: {\"data\": os.path.join(data_prefix, \"{}.npy\".format(subset)),\n",
    "                        \"latent_space\": z_file_pattern.format(config[\"dir_name\"], subset)}\n",
    "               for subset in [\"train\", \"test\"]\n",
    "             }\n",
    "\n",
    "for data_subset in io_mapping:\n",
    "    meshes = np.load(io_mapping[data_subset][\"data\"])\n",
    "    z = model.encode(data=meshes)\n",
    "    reconstructed_meshes = model.decode(z)\n",
    "    np.savetxt(io_mapping[data_subset][\"latent_space\"], z, delimiter=\",\")\n",
    "\n",
    "# reconst_errors = np.linalg.norm(x=meshes-reconstructed_meshes, axis=2)\n",
    "\n",
    "from VTK.VTKMesh import VTKObject as Mesh\n",
    "mm = Mesh(filename=\"data/vtk_meshes/1000336/output.001.vtk\")\n",
    "mm = mm.extractSubpart(subparts_ids[config[\"partition\"]])\n",
    "mesh = Mesh()\n",
    "mesh.points = reconstructed_meshes[1,:]\n",
    "mesh.triangles = mm.triangles\n",
    "mesh.SaveMeshToVTK(\"{}/vtk/{}.vtk\".format(output_dir, config[\"dir_name\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vtkplotter\n",
    "import k3d\n",
    "k3d.mesh() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt4\n",
    "# %load_ext rpy2.ipython\n",
    "# %R library(tidyverse)\n",
    "# z_train_file = io_mapping['train']['latent_space']\n",
    "# z_test_file = io_mapping['test']['latent_space']\n",
    "# %R -i z_train_file\n",
    "# %R -i z_test_file\n",
    "# %R df <- read.csv(z_train_file, header = FALSE) %>% mutate(subset=\"train\")\n",
    "# %R df <- rbind(df, read.csv(z_test_file, header = FALSE) %>% mutate(subset=\"test\"))\n",
    "# %R colnames(df) <- c(sapply(X = 1:8, FUN = function(x) paste0(\"z\", x)), \"subset\")\n",
    "# \n",
    "# %R df <- df %>% gather(key = \"latent_comp\", value = \"value\", -subset) %>% filter(latent_comp != \"z3\" & latent_comp != \"z5\")\n",
    "# %R df <- df %>% group_by(latent_comp) %>% mutate(value = scale(value, center = TRUE, scale = TRUE))\n",
    "# %R pp <- ggplot(df, mapping = aes(value, stat(density)))\n",
    "# %R pp <- pp + theme_bw() + geom_histogram()\n",
    "# %R pp <- pp + facet_grid(rows = vars(latent_comp), cols=vars(subset), scales = \"free\")\n",
    "# %R pp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_coma)",
   "language": "python",
   "name": "conda_coma"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
